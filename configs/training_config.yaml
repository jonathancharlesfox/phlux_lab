# ======================================================================
# PHLUX – STACKED VFM TRAINING CONFIGURATION (AUTOLAB-READY)
# ======================================================================
project:
  name: phlux
  client_name: ClientA
  version: v1
  unit_system: SI
  random_seed: 42

# ----------------------------------------------------------------------
# PATHS
# ----------------------------------------------------------------------
paths:
  data_root: phlux_lab/data/synthetic
  model_root: phlux_lab/models/ClientA

# ----------------------------------------------------------------------
# GLOBAL TRAINING DEFAULTS (OVERTIDEABLE PER STAGE IF DEFINED BELOW)
# ----------------------------------------------------------------------
training_defaults:
  batch_size: 128
  epochs: 150
  learning_rate: 0.001
  validation_split: 0.05
  early_stopping:
    enabled: true
    monitor: val_loss
    mode: min
    patience: 15
  loss:
    type: mse
    huber_delta: 1.0
  optimizer:
    type: adam
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
  gradient_clipping:
    enabled: false
    max_norm: 1.0
  regularization:
    dropout: 0.0
    l2: 0.0

# ----------------------------------------------------------------------
# LEARNING RATE REDUCTION (ON PLATEAU)
# ----------------------------------------------------------------------
reduce_lr_on_plateau:
  enabled: true
  monitor: val_loss
  factor: 0.5
  patience: 3
  min_lr: 1.0e-6
  cooldown: 0

# ----------------------------------------------------------------------
# PREPROCESSING & FEATURE HANDLING
# ----------------------------------------------------------------------
preprocessing:
  scaling:
    method: standard
  clipping:
    enabled: false
  noise:
    enabled: false
    features:
      suction_pressure: 0.0
      discharge_pressure: 0.0
      pump_power: 0.0
      temperature: 0.0      
      speed: 0.0
      valve_opening: 0.0
  sample_weighting:
    enabled: false
    scheme: by_target_bins
    rules:
      q_liquid:
        bins:
          - 0
          - 20
          - 40
          - 60
          - 80
          - 120
        weights:
          - 1.0
          - 1.1
          - 1.2
          - 1.4
          - 1.7
          - 2.0
      hydraulic_wear:
        bins:
          - 0.0
          - 0.02
          - 0.05
          - 0.10
        weights:
          - 1.0
          - 1.5
          - 2.0
          - 3.0
  feature_engineering:
    - name: delta_pressure
      type: subtract
      inputs:
        - discharge_pressure
        - suction_pressure
      apply_to:
        - flow
        - wear

    - name: specific_power
      type: divide
      inputs:
        - pump_power
        - q_liquid_pred
      eps: 1.0e-6
      apply_to:
        - wear

# ----------------------------------------------------------------------
# MODELS (STACKED)
# ----------------------------------------------------------------------
models:
  # ------------------------------------------------------------------
  # STAGE 1: FLOW MODEL (BASE)
  # ------------------------------------------------------------------
  flow:
    enabled: true
    data:
      train_dataset: phlux_lab/data/synthetic/ClientA_flow_clean_train.csv
      test_dataset: phlux_lab/data/synthetic/ClientA_flow_clean_test.csv
      inputs:
        - sink_pressure
        - suction_pressure
        - discharge_pressure
        - delta_pressure
        - speed
        - temperature
        - valve_opening
      targets:
        - q_liquid
      target_policy:
        q_liquid:
          transform:
            type: none
            epsilon: 1.0e-6
          clip:
            enabled: true
            min: 0.0
            max: null
    training:
      epochs: 10
      batch_size: 128
      loss:
        type: mse
    save:
      model_name: vfm_flow.keras
      preprocessor: preprocessor_flow.joblib
    evaluation:
      bins:
        enabled: true
        edges:
        - 0
        - 20
        - 40
        - 60
        - 80
        - 120
      focus_bins: [10, 80]

  # ------------------------------------------------------------------
  # STAGE 2: WEAR MODEL (STACKED ON FLOW)
  # ------------------------------------------------------------------
  wear:
    enabled: true
    enable_wear_log1p: true
    data:
      train_dataset: phlux_lab/data/synthetic/ClientA_wear_train.csv
      test_dataset: phlux_lab/data/synthetic/ClientA_wear_test.csv
      inputs:
        - sink_pressure
        - suction_pressure
        - discharge_pressure
        - speed
        - temperature
        - valve_opening
        - pump_power
      stack_inputs:
        - q_liquid_pred
      targets:
        - hydraulic_wear
      target_policy:
        hydraulic_wear:
          transform:
            type: none
            epsilon: 1.0e-6
          clip:
            enabled: false
            min: 0.0
            max: 0.15
    training:
      epochs: 150
      batch_size: 128
      loss:
        type: mae
        huber_delta: 0.01
    save:
      model_name: vfm_wear.keras
      preprocessor: preprocessor_wear.joblib
    evaluation:
      bins:
        enabled: true
        edges:
        - 0.0
        - 0.02
        - 0.05
        - 0.10
      focus_bins: [0.05, 0.1]

  # ------------------------------------------------------------------
  # STAGE 3: FLOW CORRECTION MODEL (STACKED ON FLOW + WEAR)
  # ------------------------------------------------------------------
  flow_correction:
    enabled: true
    data:
      train_dataset: phlux_lab/data/synthetic/ClientA_paired_train.csv
      test_dataset: phlux_lab/data/synthetic/ClientA_paired_test.csv
      inputs:
        - valve_opening
      stack_inputs:
        - q_liquid_pred
        - hydraulic_wear_pred
      targets:
        - q_liquid
      target_policy:
        q_liquid:
          transform:
            type: none
            epsilon: 1.0e-6
          clip:
            enabled: true
            min: 0.0
            max: null
    training:
      epochs: 50
      batch_size: 64
      regularization:
        dropout: 0.25
      lost: mae
    save:
      model_name: vfm_flow_correction.keras
      preprocessor: preprocessor_flow_correction.joblib
    evaluation:
      bins:
        enabled: true
        edges:
        - 0
        - 20
        - 40
        - 60
        - 80
        - 120
      focus_bins: [10, 80]

# ----------------------------------------------------------------------
# AUTOLAB ORCHESTRATION
# ----------------------------------------------------------------------
autolab:
  enabled: true
  regen_data: false
  max_iters: 3
  stop:
    metric: mae_by_bins
    mode: min
    min_improvement: 0.001
    patience: 2
  objective:
    primary:
      name: mae_by_bins
      target: q_liquid
      bin_min: 0.0
      bin_max: 120.0
      n_bins: 12
      focus_bins:
        - 10
        - 80
    secondary:
      # Secondary objectives are tracked as diagnostics and/or constraints (not directly optimized).
      # NOTE: Any entry with mode/value is treated as a constraint that can influence tuner actions.
      # -----------------------------------------------------------------------
      # Track (reported only)
      # -----------------------------------------------------------------------
      - name: rmse_overall
        target: q_liquid
      - name: r2_overall
        target: hydraulic_wear
      - name: r2_overall
        target: q_liquid

      # -----------------------------------------------------------------------
      # Plot-derived diagnostics used to steer actions (constraints)
      # These are computed from parity/residual behavior inside evaluate_pipeline.py.
      # Focus-range metrics use the primary.focus_bins ruler (e.g., 10–80 here).
      # -----------------------------------------------------------------------
      - name: abs_bias_focus
        target: q_liquid
        mode: max
        value: 2.0

      - name: p95_abs_error_focus
        target: q_liquid
        mode: max
        value: 6.0

      - name: corr_abs_resid_vs_true
        target: q_liquid
        mode: max
        value: 0.30

      - name: calib_slope_abs_err
        target: q_liquid
        mode: max
        value: 0.10

      - name: calib_intercept_abs_err
        target: q_liquid
        mode: max
        value: 2.0
  artifacts:
    save_run_artifacts: false
    run_root: phlux_lab/models/ClientA/runs
    save_plots: true
    save_predictions_csv: true
  tuner:
    enabled: false
    mode: llm
    history_n: 5
    max_changes: 18
    openai_model: gpt-4.1-mini
    temperature: 0.2
    fallback: rules
    allowed_paths:
      - preprocessing.sample_weighting.enabled
      - preprocessing.sample_weighting.rules.q_liquid.weights
      - training_defaults.learning_rate
      - training_defaults.epochs
      - training_defaults.batch_size
      - models.flow.training.epochs
      - models.flow.training.batch_size
      - models.flow.training.loss.type
      - models.flow.data.target_policy.q_liquid.transform.type
      - models.wear.training.epochs
      - models.wear.training.batch_size
      - models.wear.training.loss.type
      - models.wear.training.loss.huber_delta
      - models.flow_correction.training.epochs
      - models.flow_correction.training.batch_size
      - models.flow_correction.training.regularization.dropout
    bounds:
      training_defaults.learning_rate:
        - 1.0e-05
        - 5.0e-03
      training_defaults.epochs:
        - 50
        - 400
      training_defaults.batch_size:
        - 32
        - 512

      models.flow.training.epochs:
        - 50
        - 400
      models.flow.training.batch_size:
        - 32
        - 512

      models.wear.training.epochs:
        - 50
        - 400
      models.wear.training.batch_size:
        - 32
        - 512
      models.wear.training.loss.huber_delta:
        - 0.05
        - 2.0

      models.flow_correction.training.epochs:
        - 50
        - 400
      models.flow_correction.training.batch_size:
        - 32
        - 512
      models.flow_correction.training.regularization.dropout:
        - 0.0
        - 0.5

# ----------------------------------------------------------------------
# LLM CONFIGURATION (OPTIONAL / FUTURE USE)
# ----------------------------------------------------------------------
llm:
  enabled: true
  provider: openai
  model: gpt-5-mini
  temperature: 0.2
  max_output_tokens: 900
  max_calls_per_run: 6
  redact: true
  patch_contract:
    format: yaml
    mode: patch_only
    allowed_top_level_keys:
      - training_defaults
      - reduce_lr_on_plateau
      - preprocessing
      - models
